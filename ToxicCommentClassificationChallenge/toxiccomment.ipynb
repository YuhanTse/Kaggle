{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-24T02:21:59.909275Z","iopub.execute_input":"2023-02-24T02:21:59.909632Z","iopub.status.idle":"2023-02-24T02:21:59.918643Z","shell.execute_reply.started":"2023-02-24T02:21:59.909600Z","shell.execute_reply":"2023-02-24T02:21:59.917525Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow\nimport gensim","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:21:59.923213Z","iopub.execute_input":"2023-02-24T02:21:59.923751Z","iopub.status.idle":"2023-02-24T02:21:59.929428Z","shell.execute_reply.started":"2023-02-24T02:21:59.923711Z","shell.execute_reply":"2023-02-24T02:21:59.928268Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ntrain_df = pd.read_csv( '/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest_df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:21:59.931857Z","iopub.execute_input":"2023-02-24T02:21:59.932209Z","iopub.status.idle":"2023-02-24T02:22:02.131165Z","shell.execute_reply.started":"2023-02-24T02:21:59.932162Z","shell.execute_reply":"2023-02-24T02:22:02.130231Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n9  00040093b2687caa  alignment on this subject and which are contra...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  \n5             0        0       0       0              0  \n6             1        1       0       1              0  \n7             0        0       0       0              0  \n8             0        0       0       0              0  \n9             0        0       0       0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>00025465d4725e87</td>\n      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0002bcb3da6cb337</td>\n      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>00031b1e95af7921</td>\n      <td>Your vandalism to the Matt Shirvington article...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>00037261f536c51d</td>\n      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>00040093b2687caa</td>\n      <td>alignment on this subject and which are contra...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from nltk.tokenize import WordPunctTokenizer\nfrom collections import Counter\nfrom string import punctuation, ascii_lowercase\nimport regex as re\nfrom tqdm import tqdm\n\n# replace urls\nre_url = re.compile(r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\\n                    .([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\",\n                    re.MULTILINE|re.UNICODE)\n# replace ips\nre_ip = re.compile(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\")\n\n# setup tokenizer\ntokenizer = WordPunctTokenizer()\n\nvocab = Counter()\n\ndef text_to_wordlist(text, lower=False):\n    # replace URLs\n    text = re_url.sub(\"URL\", text)\n    \n    # replace IPs\n    text = re_ip.sub(\"IPADDRESS\", text)\n    \n    # Tokenize\n    text = tokenizer.tokenize(text)\n    \n    # optional: lower case\n    if lower:\n        text = [t.lower() for t in text]\n    \n    # Return a list of words\n    vocab.update(text)\n    return text\n\ndef process_comments(list_sentences, lower=False):\n    comments = []\n    for text in tqdm(list_sentences):\n        txt = text_to_wordlist(text, lower=lower)\n        comments.append(txt)\n    return comments\n\n\nlist_sentences_train = list(train_df[\"comment_text\"].fillna(\"NAN_WORD\").values)\nlist_sentences_test = list(test_df[\"comment_text\"].fillna(\"NAN_WORD\").values)\n\ncomments = process_comments(list_sentences_train + list_sentences_test, lower=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:22:02.133220Z","iopub.execute_input":"2023-02-24T02:22:02.133606Z","iopub.status.idle":"2023-02-24T02:22:22.649347Z","shell.execute_reply.started":"2023-02-24T02:22:02.133569Z","shell.execute_reply":"2023-02-24T02:22:22.648217Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"100%|██████████| 312735/312735 [00:19<00:00, 15660.72it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"from gensim.models import Word2Vec\nmodel_cbow = Word2Vec(comments, vector_size =100, window=5, min_count=5, workers=16, sg=0, negative=5) # if sg=1, we use Skip-gram","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:28:35.277533Z","iopub.status.idle":"2023-02-24T02:28:35.278008Z","shell.execute_reply.started":"2023-02-24T02:28:35.277764Z","shell.execute_reply":"2023-02-24T02:28:35.277787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_cbow.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:24:40.038144Z","iopub.execute_input":"2023-02-24T02:24:40.038501Z","iopub.status.idle":"2023-02-24T02:24:40.073405Z","shell.execute_reply.started":"2023-02-24T02:24:40.038465Z","shell.execute_reply":"2023-02-24T02:24:40.072183Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"[('prince', 0.9814999103546143),\n ('princess', 0.9668576717376709),\n ('queen', 0.951403021812439),\n ('mary', 0.9439194798469543),\n ('bishop', 0.9320930242538452),\n ('mayor', 0.9300081133842468),\n ('abraham', 0.9268338680267334),\n ('duchess', 0.9258257746696472),\n ('hrh', 0.9180528521537781),\n ('victoria', 0.9164304137229919)]"},"metadata":{}}]},{"cell_type":"code","source":"MAX_NB_WORDS = len(model_cbow.wv.key_to_index)\nMAX_SEQUENCE_LENGTH = 200\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nword_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}\nsequences = [[word_index.get(t, 0) for t in comment]\n             for comment in comments[:len(list_sentences_train)]]\ntest_sequences = [[word_index.get(t, 0)  for t in comment] \n                  for comment in comments[len(list_sentences_train):]]\n\n# pad\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, \n                     padding=\"pre\", truncating=\"post\")\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train_df[list_classes].values\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', y.shape)\n\ntest_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",\n                          truncating=\"post\")\nprint('Shape of test_data tensor:', test_data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:24:40.075068Z","iopub.execute_input":"2023-02-24T02:24:40.075432Z","iopub.status.idle":"2023-02-24T02:24:48.045022Z","shell.execute_reply.started":"2023-02-24T02:24:40.075398Z","shell.execute_reply":"2023-02-24T02:24:48.044070Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Shape of data tensor: (159571, 200)\nShape of label tensor: (159571, 6)\nShape of test_data tensor: (153164, 200)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nWV_DIM = 100\nnb_words = min(MAX_NB_WORDS, len(model_cbow.wv.key_to_index))\n# 我们首先先随机初始化嵌入矩阵\nwv_matrix = (np.random.rand(nb_words, WV_DIM) - 0.5) / 5.0\n# 然后尝试地将已经学习好的词向量覆盖掉矩阵的对应元素\nfor word, i in word_index.items():\n    if i >= MAX_NB_WORDS:\n        continue\n    try :\n        embedding_vector = model_cbow.wv.get_vector(word)\n        wv_matrix[i] = embedding_vector\n    except:\n        print('can not find word', word)\n        pass     ","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:24:48.048221Z","iopub.execute_input":"2023-02-24T02:24:48.048542Z","iopub.status.idle":"2023-02-24T02:24:48.277838Z","shell.execute_reply.started":"2023-02-24T02:24:48.048513Z","shell.execute_reply":"2023-02-24T02:24:48.276807Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout,SpatialDropout1D, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import BatchNormalization\n\nwv_layer = Embedding(nb_words,\n                     WV_DIM,\n                     mask_zero=False,\n                     weights=[wv_matrix], # 在这里，我们用之前构建的wv_matrix作为初始参数的设定\n                     input_length=MAX_SEQUENCE_LENGTH,\n                     trainable=False)\n\n# Inputs\ncomment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = wv_layer(comment_input)\n\n# biLSTM\nembedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\nx = Bidirectional(LSTM(64, return_sequences=False))(embedded_sequences)\n\n# Output\nx = Dropout(0.2)(x)\nx = BatchNormalization()(x)\npreds = Dense(6, activation='sigmoid')(x)\n\n# build the model\nmodel = Model(inputs=[comment_input], outputs=preds)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=Adam(learning_rate=0.001, clipnorm=.25, beta_1=0.7, beta_2=0.99),\n              metrics=[])\n              \nhist = model.fit([data], y, validation_split=0.1, epochs=10, batch_size=256, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:24:48.279341Z","iopub.execute_input":"2023-02-24T02:24:48.279920Z","iopub.status.idle":"2023-02-24T02:24:48.288165Z","shell.execute_reply.started":"2023-02-24T02:24:48.279881Z","shell.execute_reply":"2023-02-24T02:24:48.287217Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"y_test = model.predict(test_data)\ny_test","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:24:48.291197Z","iopub.execute_input":"2023-02-24T02:24:48.291633Z","iopub.status.idle":"2023-02-24T02:24:48.298411Z","shell.execute_reply.started":"2023-02-24T02:24:48.291600Z","shell.execute_reply":"2023-02-24T02:24:48.297532Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"y_test = pd.DataFrame(round(y_test))","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:24:48.299651Z","iopub.execute_input":"2023-02-24T02:24:48.300545Z","iopub.status.idle":"2023-02-24T02:24:48.307301Z","shell.execute_reply.started":"2023-02-24T02:24:48.300510Z","shell.execute_reply":"2023-02-24T02:24:48.306435Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')\nsubmission.iloc[:,1:] = y_test\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:24:48.311263Z","iopub.execute_input":"2023-02-24T02:24:48.311524Z","iopub.status.idle":"2023-02-24T02:24:48.317788Z","shell.execute_reply.started":"2023-02-24T02:24:48.311501Z","shell.execute_reply":"2023-02-24T02:24:48.316902Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# wv_layer = Embedding(nb_words,\n#                      WV_DIM,\n#                      mask_zero=False,\n#                      # weights=[wv_matrix],\n#                      input_length=MAX_SEQUENCE_LENGTH,\n#                      trainable=False)\n\n# # Inputs\n# comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n# embedded_sequences = wv_layer(comment_input)\n\n# # biLSTM\n# embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\n# x = Bidirectional(LSTM(64, return_sequences=False))(embedded_sequences)\n\n# # Output\n# x = Dropout(0.2)(x)\n# x = BatchNormalization()(x)\n# preds = Dense(6, activation='sigmoid')(x)\n\n# # build the model\n# model = Model(inputs=[comment_input], outputs=preds)\n# model.compile(loss='binary_crossentropy',\n#               optimizer=Adam(learning_rate=0.001, clipnorm=.25, beta_1=0.7, beta_2=0.99),\n#               metrics=[])\n\n# hist_ = model.fit([data], y, validation_split=0.1,\n#                  epochs=10, batch_size=256, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:24:48.319248Z","iopub.execute_input":"2023-02-24T02:24:48.319736Z","iopub.status.idle":"2023-02-24T02:27:53.204560Z","shell.execute_reply.started":"2023-02-24T02:24:48.319701Z","shell.execute_reply":"2023-02-24T02:27:53.203628Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Epoch 1/10\n561/561 [==============================] - 22s 33ms/step - loss: 0.1792 - val_loss: 0.1244\nEpoch 2/10\n561/561 [==============================] - 18s 32ms/step - loss: 0.1073 - val_loss: 0.1045\nEpoch 3/10\n561/561 [==============================] - 18s 32ms/step - loss: 0.1043 - val_loss: 0.0996\nEpoch 4/10\n561/561 [==============================] - 18s 32ms/step - loss: 0.0988 - val_loss: 0.0972\nEpoch 5/10\n561/561 [==============================] - 18s 32ms/step - loss: 0.0934 - val_loss: 0.0915\nEpoch 6/10\n561/561 [==============================] - 18s 32ms/step - loss: 0.0885 - val_loss: 0.0857\nEpoch 7/10\n561/561 [==============================] - 18s 32ms/step - loss: 0.0857 - val_loss: 0.0826\nEpoch 8/10\n561/561 [==============================] - 18s 32ms/step - loss: 0.0829 - val_loss: 0.0834\nEpoch 9/10\n561/561 [==============================] - 18s 32ms/step - loss: 0.0806 - val_loss: 0.0876\nEpoch 10/10\n561/561 [==============================] - 18s 32ms/step - loss: 0.0787 - val_loss: 0.0822\n","output_type":"stream"}]},{"cell_type":"code","source":"# y_test = model.predict(test_data)","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:27:53.206575Z","iopub.execute_input":"2023-02-24T02:27:53.206951Z","iopub.status.idle":"2023-02-24T02:28:35.240706Z","shell.execute_reply.started":"2023-02-24T02:27:53.206911Z","shell.execute_reply":"2023-02-24T02:28:35.239692Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"4787/4787 [==============================] - 34s 7ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# y_test = pd.DataFrame(np.round(y_test))","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:29:42.805269Z","iopub.execute_input":"2023-02-24T02:29:42.805631Z","iopub.status.idle":"2023-02-24T02:29:42.816898Z","shell.execute_reply.started":"2023-02-24T02:29:42.805600Z","shell.execute_reply":"2023-02-24T02:29:42.815904Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# submission = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')\n# submission.iloc[:,1:] = y_test\n# submission.to_csv('/kaggle/working/submission2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-24T02:30:03.652287Z","iopub.execute_input":"2023-02-24T02:30:03.652646Z","iopub.status.idle":"2023-02-24T02:30:04.168797Z","shell.execute_reply.started":"2023-02-24T02:30:03.652614Z","shell.execute_reply":"2023-02-24T02:30:04.167523Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}